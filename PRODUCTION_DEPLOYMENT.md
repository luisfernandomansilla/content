# Production Deployment Guide

## ðŸš€ Environment Configuration

### Development vs Production

Content Creator automatically configures itself based on the `ENVIRONMENT` variable:

#### Development (Default)
```bash
ENVIRONMENT=development
HOST=127.0.0.1
PORT=7860
```

#### Production
```bash
ENVIRONMENT=production
HOST=0.0.0.0
PORT=80
```

### Configuration Files

1. **Copy example configuration:**
   ```bash
   cp .env.example .env
   ```

2. **Edit `.env` for production:**
   ```bash
   # Production settings
   ENVIRONMENT=production
   HOST=0.0.0.0
   PORT=80
   
   # Your actual tokens
   HF_TOKEN=your_real_huggingface_token_here
   CIVITAI_API_TOKEN=your_real_civitai_token_here
   
   # Performance settings
   CLEANUP_TEMP_FILES=true
   AUTO_OPTIMIZE_SETTINGS=true
   ```

## ðŸ”§ FLUX+LoRA Compatibility Fix

### Issue Resolved
- **Problem**: FLUX models with LoRA (like `Flux-NSFW-uncensored`) were generating gradient images instead of actual content
- **Root Cause**: XFormers incompatibility with FLUX+LoRA causing `attn_output` reference errors
- **Solution**: Automatic XFormers disabling for FLUX models

### Technical Details

The system now automatically:
- âœ… Detects FLUX and FLUX+LoRA models
- âœ… Disables XFormers for these models
- âœ… Uses standard attention mechanisms
- âœ… Maintains performance with other optimizations

## âš¡ Pipeline Caching System

### Performance Enhancement
- **Problem**: Models were being loaded from scratch on every generation (2-5 minutes each time)
- **Solution**: Intelligent pipeline caching system
- **Benefit**: First load takes time, subsequent generations are instant

### How It Works

**First Generation:**
```
INFO:src.content_creator.image_generator:ðŸ”„ Loading new pipeline for Flux-NSFW-uncensored
[Loading process takes 2-5 minutes]
INFO:src.content_creator.image_generator:âœ… Pipeline cached for Flux-NSFW-uncensored
```

**Subsequent Generations:**
```
INFO:src.content_creator.image_generator:ðŸ“‹ Using cached pipeline for Flux-NSFW-uncensored
[Generation starts immediately]
```

### Cache Features
- âœ… **Automatic caching**: Models cached after first load
- âœ… **Memory management**: Efficient GPU memory usage
- âœ… **Multi-model support**: Different models cached separately
- âœ… **Manual clearing**: Cache can be cleared to free memory

### Resolution Optimization
- âœ… **FLUX models**: Dimensions automatically rounded to multiples of 16
- âœ… **Other models**: Dimensions rounded to multiples of 8
- âœ… **No manual adjustment**: System handles compatibility automatically

### Logs to Expect

**Successful FLUX+LoRA Loading:**
```
INFO:src.content_creator.image_generator:âœ… Base FLUX model loaded without variant!
INFO:src.content_creator.image_generator:ðŸ”„ Loading LoRA weights: lora.safetensors
INFO:src.content_creator.image_generator:âœ… LoRA weights loaded successfully!
INFO:src.content_creator.image_generator:ðŸš« Xformers disabled for FLUX+LoRA model (compatibility)
INFO:src.content_creator.image_generator:ðŸ”§ Using standard attention mechanism for better stability
```

## ðŸŒ Deployment Options

### 1. Direct Python Deployment
```bash
# Set production environment
export ENVIRONMENT=production
export PORT=80
export HOST=0.0.0.0

# Run the application
python main.py
```

### 2. Docker Deployment
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

# Production environment variables
ENV ENVIRONMENT=production
ENV HOST=0.0.0.0
ENV PORT=80

EXPOSE 80

CMD ["python", "main.py"]
```

**Run Docker container:**
```bash
docker build -t content-creator .
docker run -p 80:80 \
  -e HF_TOKEN=your_token \
  -e CIVITAI_API_TOKEN=your_token \
  content-creator
```

### 3. systemd Service (Linux)
```ini
[Unit]
Description=Content Creator AI Service
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/path/to/content-creator
Environment=ENVIRONMENT=production
Environment=HOST=0.0.0.0
Environment=PORT=80
Environment=HF_TOKEN=your_token_here
ExecStart=/usr/bin/python3 main.py
Restart=always

[Install]
WantedBy=multi-user.target
```

## ðŸ”’ Security Considerations

### API Tokens
- Store tokens in `.env` file (automatically ignored by git)
- Use environment variables in production
- Restrict token permissions to minimum required

### Network Security
- Use reverse proxy (nginx/Apache) for HTTPS
- Configure firewall rules
- Consider rate limiting

### Example nginx configuration:
```nginx
server {
    listen 443 ssl;
    server_name yourdomain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        proxy_pass http://localhost:80;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

## ðŸ§ª Testing Your Deployment

Run the compatibility tests:
```bash
python test_flux_fix.py
```

Expected output:
```
ðŸš€ Content Creator - Compatibility Tests
========================================================

ðŸ§ª Testing FLUX+LoRA Compatibility Fix
==================================================
ðŸ” Testing model: Flux-NSFW-uncensored
âœ… Model found: flux_lora
ðŸ“‹ Base model: black-forest-labs/FLUX.1-dev
ðŸš€ Testing pipeline loading...
âœ… Pipeline loaded successfully!
ðŸ”§ XFormers properly disabled for FLUX+LoRA

ðŸŒ Testing Production Configuration
==================================================
ðŸ”§ Environment: production
ðŸŒ Host: 0.0.0.0
ðŸ”Œ Port: 80
ðŸŽ¨ Theme: soft
âœ… Production configuration looks correct!

ðŸ“Š Test Results Summary
==============================
âœ… Passed: 2/2
ðŸŽ‰ All tests passed! The fixes are working correctly.
```

## ðŸš¨ Troubleshooting

### Common Issues

1. **Port 80 Permission Denied**
   - Run as root: `sudo python main.py`
   - Or use port > 1024: `PORT=8080`

2. **XFormers Still Causing Issues**
   - Check logs for "Xformers disabled" messages
   - Ensure latest code is deployed

3. **Token Authentication Errors**
   - Verify token validity on HuggingFace
   - Check token permissions for model access

4. **Memory Issues**
   - Monitor GPU/CPU memory usage
   - Adjust `MAX_CONCURRENT_GENERATIONS=1`
   - Enable `CLEANUP_TEMP_FILES=true`

### Debug Mode
```bash
DEBUG=true python main.py
```

This will show detailed logs including:
- Model loading process
- Optimization decisions
- Token usage (partially masked)
- Performance metrics

## ðŸ“Š Performance Monitoring

### Key Metrics to Monitor
- Model loading time (first time: 2-5 minutes)
- Generation time (30s-2min after warmup)
- Memory usage (GPU/CPU)
- Disk space (model cache growth)

### Optimization Tips
- Keep `AUTO_OPTIMIZE_SETTINGS=true`
- Use SSD storage for model cache
- Ensure adequate GPU memory
- Monitor temperature under load

## ðŸ”„ Updates and Maintenance

### Updating the Application
```bash
git pull origin main
pip install -r requirements.txt --upgrade
```

### Clearing Model Cache
```bash
rm -rf models/models--*/
```

### Backup Important Data
- Configuration files (`.env`)
- Generated outputs (`outputs/`)
- Custom model configurations 